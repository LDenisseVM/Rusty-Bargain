# El servicio de venta de autos usados Rusty Bargain está desarrollando una aplicación para atraer nuevos clientes. Gracias a esa app, puedes averiguar rápidamente el valor de mercado de tu coche. Tienes acceso al historial: especificaciones técnicas, versiones de equipamiento y precios. Tienes que crear un modelo que determine el valor de mercado.
# A Rusty Bargain le interesa:
# - la calidad de la predicción;
# - la velocidad de la predicción;
# - el tiempo requerido para el entrenamiento

# ## Preparación de datos

# Importar librerias


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OrdinalEncoder
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb
import time


# Examinar los datos


data= pd.read_csv('/datasets/car_data.csv')


data.info()

data.head()

data.describe()


# Vemos valores absurdos, asi que los abordamos 
# 
# Empezamos con los de Registration Year
# Nos quedaremos con los datos que su año de registro es menor a 2024, pues es el año actual, y para evitar valores problematicos, eliminaremos el 1% mas bajo


data['RegistrationYear'].quantile(.001)

data['RegistrationYear']= data['RegistrationYear'].mask((data['RegistrationYear'] >= 2024) & (data['RegistrationYear'] <= data['RegistrationYear'].quantile(.001)), np.nan)

data.sort_values('RegistrationYear').head(10)

data.info()


# Y hacemos lo mismo con Power

data['Power'].quantile(.1)


# Vemos que el 10%  muestra valores de 0 cv, lo más probable es que sea un error, asi que probamos con el 15%

data['Power'].quantile(.15)


# Y vemos que este valor es más razonable, asi que usaremos esta medida, y checamos lo mismo con los máximos

data['Power'].quantile(.99)

# Asi que eliminaremos ese último 1%

data['Power']= data['Power'].mask((data['Power'] <= data['Power'].quantile(.15)) & (data['Power'] >= data['Power'].quantile(.99)), np.nan)

data.info()


# Y lo mismo con Price


data.sort_values('Price').head(1000)


data['Price'].quantile(.01)


# Vemos un problema con Price, pues no es posible que vendan un carro con un precio de $0, asi que buscamos un precio más razonable

# Esto parece un poco más razonable, asi que eliminaremos ese 5%

data = data[(data['Price'] >= data['Price'].quantile(.05))]


# Revisamos la data

data.info()

data.describe()


# Y vemos que ahora tenemos datos mucho más coherentes

# Asi que ahora pasaremos a tratar los valores ausentes

# Empezaremos eliminando todas las columnas que no son relevantes para construir nuestro modelo predictor 


data= data.drop(['DateCrawled', 'RegistrationMonth', 'DateCreated', 'NumberOfPictures', 'PostalCode', 'LastSeen'], axis=1)


data.info()


# Crearemos una data, sin valores nulos, para poder rellenar lo que podamos con ella 


data2=data.dropna()
data2.info()


data[data['VehicleType'].isna()]


# Creamos una función que rellene los valores ausentes con la moda de tipo de vehiculo según el modelo, por lo que eliminaremos las filas dónde hay valor ausente en "Model"

data= data.dropna(subset=['Model'])


VehicleType_grouped = data2.groupby('Model')['VehicleType'].agg(lambda x: x.mode()[0])


def fill_vehicle_type(row):
    vehicle_type = row['VehicleType']
    model = row['Model']
    if pd.isna(vehicle_type):
        return VehicleType_grouped.get(model)
    else:
        return vehicle_type

data['VehicleType'] = data.apply(fill_vehicle_type, axis=1)


data.info()

data[data['VehicleType'].isna()]


# Hacemos lo mismo con 'FuelType'

FuelType_grouped = data2.groupby('VehicleType')['FuelType'].agg(lambda x: x.mode()[0])


def fill_fuel_type(row):
    vehicle_type = row['VehicleType']
    fueltype = row['FuelType']
    if pd.isna(fueltype):
        return FuelType_grouped.get(vehicle_type)
    else:
        return fueltype

data['FuelType'] = data.apply(fill_fuel_type, axis=1)

Gearbox_grouped = data2.groupby('Model')['Gearbox'].agg(lambda x: x.mode()[0])


def fill_gearbox(row):
    Model = row['Model']
    Gearbox = row['Gearbox']
    if pd.isna(Gearbox):
        return Gearbox_grouped.get(Model)
    else:
        return Gearbox

data['Gearbox'] = data.apply(fill_gearbox, axis=1)



data.info()


# Ahora rellenaremos con "Unknown" todas las demás columnas que también tengan valores ausentes


data= data.fillna('Unknown')


# El único que tiene un tipo de dato diferente al que debería tener es Registration Year, sin embargo, para nuestro modelo es mejor un tipo de objeto númerico, asi que lo dejaremos así


data.info()
data.head()


# Ahora que tenemos listos los datos, simplemente transformaremos los nombres de las columnas a snake_case, y ahora si empezaremos con la creación del modelo

data.columns = ['Price', 'Vehicle_Type', 'Registration_Year', 'Gearbox', 'Power', 'Model', 'Mileage', 'Fuel_Type', 'Brand', 'Not_Repaired']


# Arreglar valores nulos, valores anormales, tipo de datos


# ## Entrenamiento del modelo 

# Entrena diferentes modelos con varios hiperparámetros (debes hacer al menos dos modelos diferentes, pero más es mejor. Recuerda, varias implementaciones de potenciación del gradiente no cuentan como modelos diferentes). El punto principal de este paso es comparar métodos de potenciación del gradiente con bosque aleatorio, árbol de decisión y regresión lineal.

# Convertiremos los datos a númericos pues es una tarea de regresión, lo haremos con OHE, y con etiqueta para ver cual funciona mejor

data= data.reset_index(drop=True)

# OHE

data_ohe= pd.get_dummies(data, drop_first=True)

data_ohe.head()


# Ordinal Encoder

data_category= data[['Vehicle_Type', 'Gearbox', 'Model', 'Fuel_Type', 'Brand', 'Not_Repaired']]

data_numeric= data[['Price', 'Registration_Year', 'Power', 'Mileage']]

encoder= OrdinalEncoder()


data_category_ordinal=  pd.DataFrame(encoder.fit_transform(data_category), columns=data_category.columns)

data_ordinal= data_category_ordinal.join(data_numeric, how='outer')

data_ordinal.info()

data_ordinal.head()

data_ordinal=data_ordinal.astype('int')


data.head()


# Separamos los datos para entrenamiento y prueba


features_ohe= data_ohe.drop('Price', axis=1)
target= data['Price']
features_train_ohe, features_test_ohe, target_train, target_test= train_test_split(features_ohe, target, test_size=.40, random_state=12345)
features_valid_ohe, features_test_ohe, target_valid, target_test= train_test_split(features_test_ohe, target_test, test_size=.20, random_state=12345)



features_ordinal= data_ordinal.drop('Price', axis=1)
features_train_ordinal, features_test_ordinal= train_test_split(features_ordinal, test_size=.40, random_state=12345)
features_valid_ordinal, features_test_ordinal= train_test_split(features_test_ordinal, test_size=.20, random_state=12345)



features= data.drop('Price', axis=1)

features_train, features_test= train_test_split(features, test_size=.40, random_state=12345)
features_valid, features_test= train_test_split(features_test, test_size=.20, random_state=12345)


# Usé solo una fracción de los datos al crear el código, pues al ser demasiados, tardaba mucho en correr, lo dejé en markdown por si hay que hacer alguna corrección, pueda volver a ponerlo 

# features_train_ordinal=features_train_ordinal.sample(frac=.1, random_state=12345) features_test_ordinal=features_test_ordinal.sample(frac=.1, random_state=12345)
# features_train_ohe=features_train_ohe.sample(frac=.1, random_state=12345) features_test_ohe=features_test_ohe.sample(frac=.1, random_state=12345)
# target_train=target_train.sample(frac=.1, random_state=12345) target_test=target_test.sample(frac=.1, random_state=12345)
# features_train=features_train.sample(frac=.1, random_state=12345) features_test=features_test.sample(frac=.1, random_state=12345)
# features_valid_ohe=features_valid_ohe.sample(frac=.1, random_state=12345)
# features_valid_ordinal=features_valid_ordinal.sample(frac=.1, random_state=12345) target_valid=target_valid.sample(frac=.1, random_state=12345)

model_rf = RandomForestRegressor(random_state=12345, n_estimators=50, max_depth=10)
model_rf.fit(features_train_ordinal, target_train)
predictions_rf = model_rf.predict(features_test_ordinal)
mse = mean_squared_error(target_test, predictions_rf)  # Primero calculamos el MSE
rmse = np.sqrt(mse)
rmse


model_lr=LinearRegression()
    
model_lr.fit(features_train_ordinal, target_train)
predictions_lr=model_lr.predict(features_test_ordinal)
    

    
RECM= mean_squared_error(target_test, predictions_lr, squared=False)
RECM


# Vemos que el RECM es más alto en el modelo de Regresión Lineal


params = {
    'objective':'mse',
    'boosting_type': 'gbdt',
    'metric': 'rmse',
    'learning_rate': 0.05,
    'feature_fraction': 0.9
}

# Crear el conjunto de datos LightGBM
lgb_train = lgb.Dataset(features_train_ohe, target_train)
lgb_test = lgb.Dataset(features_test_ohe, target_test)

# Entrenar el modelo LightGBM
num_boost_round = 1000
bst = lgb.train(params, lgb_train, num_boost_round, early_stopping_rounds=10, valid_sets=lgb_test)



params = {
    'objective':'mse',
    'boosting_type': 'gbdt',
    'metric': 'rmse',
    'learning_rate': 0.05,
    'feature_fraction': 0.9
}

# Crear el conjunto de datos LightGBM
lgb_train = lgb.Dataset(features_train_ordinal, target_train)
lgb_test = lgb.Dataset(features_test_ordinal, target_test)

# Entrenar el modelo LightGBM
num_boost_round = 1000
bst = lgb.train(params, lgb_train, num_boost_round, early_stopping_rounds=10, valid_sets=lgb_test)


params = {
    'objective':'mse',
    'boosting_type': 'gbdt',
    'metric': 'rmse',
    'learning_rate': 0.1
}

# Crear el conjunto de datos LightGBM
lgb_train = lgb.Dataset(features_train_ordinal, target_train)
lgb_test = lgb.Dataset(features_test_ordinal, target_test)

# Entrenar el modelo LightGBM
num_boost_round = 10000
bst = lgb.train(params, lgb_train, num_boost_round, early_stopping_rounds=50, valid_sets=lgb_test)



params = {
    'objective':'mse',
    'boosting_type': 'gbdt',
    'metric': 'rmse',
    'learning_rate': 0.1
}

# Crear el conjunto de datos LightGBM
lgb_train = lgb.Dataset(features_train_ohe, target_train)
lgb_test = lgb.Dataset(features_test_ohe, target_test)

# Entrenar el modelo LightGBM
num_boost_round = 10000
bst = lgb.train(params, lgb_train, num_boost_round, early_stopping_rounds=50, valid_sets=lgb_test)


# Vemos que el rmse más bajo es para el modelo con descenso de gradiente con las caracteristicas con formato ohe

# ## Análisis del modelo

# Random Forest Regressor:

# Calidad de la predicción:


predictions_rf_valid=model_rf.predict(features_valid_ordinal)


rmse_rf= mean_squared_error(target_valid, predictions_rf_valid, squared=False)


# Velocidad de la predicción:

start_time = time.time()
predictions = model_rf.predict(features_valid_ordinal)
end_time = time.time()

time_pred_rf= end_time - start_time


# Tiempo requerido para el entrenamiento:

start_time = time.time()
fit = model_rf.fit(features_train_ordinal, target_train)
end_time = time.time()

time_fit_rf= end_time - start_time



rf_data = ['Random Forest', rmse_rf, time_pred_rf, time_fit_rf]


# Linear Regressor:

# Calidad de la predicción:

predictions_lr_valid=model_lr.predict(features_valid_ordinal)

rmse_lr=mean_squared_error(target_valid, predictions_lr_valid, squared=False)


# Velocidad de la predicción:


start_time = time.time()
predictions = model_lr.predict(features_valid_ordinal)
end_time = time.time()

time_pred_lr= end_time - start_time


# Tiempo requerido para el entrenamiento:


start_time = time.time()
fit = model_lr.fit(features_train_ordinal, target_train)
end_time = time.time()

time_fit_lr= end_time - start_time



lr_data=['LinearRegression', rmse_lr, time_pred_lr, time_fit_lr]


# Light GBM:

# Calidad de la predicción:


bst_valid_predictions = bst.predict(features_valid_ohe, num_iteration=bst.best_iteration)


rmse_bst=mean_squared_error(target_valid, bst_valid_predictions, squared=False)


# Velocidad de la predicción:


start_time = time.time()
predictions = bst.predict(features_valid_ohe, num_iteration=bst.best_iteration)
end_time = time.time()

time_pred_bst= end_time - start_time


# Tiempo requerido para el entrenamiento:


start_time = time.time()
fit = lgb.train(params, lgb_train, num_boost_round, early_stopping_rounds=50, valid_sets=lgb_test)
end_time = time.time()

time_fit_bst= end_time - start_time


bst_data=['LightGbm', rmse_bst, time_pred_bst, time_fit_bst]


model_data= [rf_data, lr_data, bst_data]
list_columns= ['Model', 'RMSE', 'Time_pred', 'Time_fit']
models_info= pd.DataFrame(data= model_data, columns= list_columns)


models_info


# Vemos que el modelo con mayor cálidad en cuanto a predicción es Light GBM, pues es el que tiene el RMSE más bajo, sin embargo también es el que tarda más haciendo predicciones y con el entrenmiento, mientras que Linear Regression es el modelo más rápido, pero con peor cálidad de los 3, pues tiene el RMSE más alto 
